---
template: main.html
tags:
  - Service Mesh
  - 注册中心
  - 负载均衡
  - 服务治理
---

# 服务注册中心

## 健康检查设计

- 服务主动探活
  - 服务定时发送租约信息给注册中心
  - 写操作压力大、主动租约不能保证服务健康
- 注册中心发起健康检查
  - 可能会发生旧服务重新激活的问题
  - 可用 envory 做服务名检查
- 调用房的负载均衡器进行健康检查
  - 如 grpc 的自动摘除节点
  - IP 重用产生脏数据

## 注册中心的选型

| 特征         | Nacos | Eureka | Zookeeper  | Consul | Etcd   |
| ------------ | ----- | ------ | ---------- | ------ | ------ |
| 一致性协议   | AP    | AP     | CP         | CP     | CP     |
| 健康检查     | 多种  | TTL    | Keep Alive | 多种   | TTL    |
| 网络异常保护 | 支持  | 支持   | 不支持     | 支持   | 不支持 |
| 实现语言     | Java  | Java   | Java       | Go     | Go     |

## 搭建高可用的注册中心

- Q1：注册中心故障了，服务是否还能正常访问？
  - 只要服务缓存了服务数据，影响面可控
  - 服务无法扩容
- Q2：注册中心因为高负载，推送了异常的数据，服务是否还能正常访问？
  - 在客户端的服务发现 SDk 中加入自我保护机制，当服务的节点数量下降超过一定阈值，就进入保护机制，放弃使用新推送过来的服务注册信息
- Q3：新加入的机器，出现了网络联通性问题（注册中心与服务网络正常，服务之间网络异常），应该怎样应对？
  - 在负载均衡中加入被动健康检查（节点熔断）和主动健康检查，主动剔除失效的节点
- Q4：服务是否应该完全信任注册中心推送的数据？

  - 相比注册中心数据，更信任本地数据
  - 使用 Envoy 2X2 矩阵来决定节点是否应该路由

    | 发现状态 | 健康检查成功 | 健康检查失败 |
    | -------- | ------------ | ------------ |
    | 发现     | 路由         | 不路由       |
    | 未发现   | 路由         | 不路由，删除 |

- Q5：服务发布后，造成 N\*M 次事件通知，形成广播风暴，该如何解决？
  - 将消息推送合并

## Service Mesh 中的注册中心

- 涉及跨集群访问
  - EDS 服务发现规范
- 与 K8S 集群结合
  - 需要注意节点 IP 变化的问题

## Service Mesh 中的注册中心的优势

- 由 sidercar 代理注册，减少服务开发量，可以控制注册 meta 信息一致化
- 通过控制面聚合多种、多个注册中心数据，降低单一注册中心的读写压力，更容易水平扩展
- 通过 sidercar 提供服务正确性检查功能，通过 header 中增加服务名与本地服务名称做校验

# 负载均衡

## 应该选择什么样的负载均衡器

- 硬件负载均衡器

  - F5
  - Citrix NetScaler
  - radware
  - Array

- DNS 负载均衡
- 软件负载均衡

  - Nginx（七层）
  - HAProxy（七层）
  - LVS（四层）

- 程序内负载均衡：讲负载均衡放在服务程序内部
- Service Mesh 负载均衡：利用 sidercar 做负载均衡，属于软件负载均衡的一种

## 负载算法

- Round Robin：轮训算法，适合节点权重一致的场景
- Weighted Round Robin：将权重大的节点分散开，取最大公约数，做简单轮训；在服务重启时会出现请求集中的现象
- Weighted Random：通过随机的方式进行负载，配合二分查找，将时间复杂度降低到 logn，对后端节点非常均衡
- Two Random Choices：通过两次随机算法，选取两个节点，再对比两个节点的负载、延时等信息，选取最优的一个
- Stick Session：同一个客户端，负载到的节点相同，负载不均衡，慎用

## 服务发现后如何实现节点保护

- 主动健康检查
  - 在负载均衡中主动健康检查。
  - 会产生大量无用 ping；建议选择获取过少节点时才触发，如当前节点数比十五分钟前的少 20%时触发
- 恐慌阈值
  - 当健康检查后，可用的节点依然小于阈值（10%），则忽略健康检查的结果，把流量负载到所有节点，包括异常的节点，保证理由的均衡，健康的节点也不会因为激增的流量而雪崩
- 被动健康检查
  - 通过正常的流量判断节点是否正常，也就是利用节点熔断器
  - 将 499 及以上的错误码认为是后端服务异常，统计 10 秒滑动窗口内异常请求占比

## 节点染色

- 利用节点上的标签进行流量路由；把某一类节点打上相同标签，负载均衡器根据相同标签分发流量
  - 金丝雀发布、A/B 测试、故障演练、流量分区等
- 如何操作
  - 在网关层，根据 header 信息或者权重对流量进行染色
  - 在注册中心中也要写入对应的 metaData 信息，用户在负载均衡层进行流量过滤

## 地域优先访问

- 在 Envoy 中被称为 zone 感知路由
- 名词解释
  - 始发集群：调用方，Client 的服务节点集群
  - 上游集群：被调用方，Server 服务节点集群
  - zone：区域（Region）和可用区（Availability Zone）
- zone 感知路由，会根据始发集群的所在区节点数量和目标集群的所在区节点数量，动态计算一个相对的比例；通过动态计算上下游的节点数，将流量正确的路由到各个分区，避免上游集群承受过大的访问量而崩溃
  - 始发集群的本地 zone 节点数量小于或等于上游服务节点数量，直接全量负载到上游集群 zone，并计算剩余流量比例用于服务其它 zone
  - 始发集群的本地 zone 节点数量大于上游服务节点数量，需要将剩余流量路由到其它上游服务 zone
- 节点缩量变化，可能会导致瞬间的流量不均衡
  - P2C（pick off to chioce）

## 延时、负载加权（EWMA）

- 基于 P2C 算法的负载均衡，如 Envoy、Nginx、Linkerd
- 负载率: client_success/server_cpu\*math.Sqrt(latency+1)\*(inflight+1)
  - client_success：客户端请求成功率
  - server_cpu：每次请求 response 的 header 携带的瞬时 cpu
  - latency：客户端计算的延时
  - inflight：正在发送中的请求数量
- EWMA 计算延时和客户端成功率

## 负载均衡中的常见问题

- Q1：为什么四层负载均衡流量不均匀？
  - 四层负载均衡是基于连接做负载均衡，由于连接保持，流量会始终打到固定节点；新加入的节点，往往需要较长时间才有流量
- Q2：负载流量一致，后端服务的负载就一致吗？
  - 因为服务器硬件差异，后端服务的负载很难一致
- Q3：节点下线后，如何及时摘除节点？
  - 注册中心异步推送存在延迟，可通过 upsteam 节点，返回健康检查信息检查失败的头信息，快速摘掉节点

# 使用路由器模块（软路由）针对不同流量实现不同的路由策略

## 名词解释

- Name：主要对应 Listener 的名称，用端口座位名字，可以保证唯一性
- Domains：在 Envoy 中会先做一次初步的过滤，这层过滤就是服务域名。主要通过字符串匹配
- routes：服务路由配置，针对要访问的服务设置多条路由配置
  - match：路由匹配，匹配此字段到对应的路由设置，可以设置如 path、pathPrefix、header 等
  - route：基本规则设置，如 Cluster 字段代表路由对应的服务名
  - per_filter_config：路由对应的中间件配置，用于服务治理、限流、熔断等

## 路由中间件

- 把中间件放在路由层

## 服务重写

- sidercar 的设计模式，保证基础设施和业务独立演化

## RDS 服务发现服务

- 动态发现路由变化，及时更新路由策略

# 服务治理

## 限流算法

- 计数器：记录一定时间内的请求量
  - 在临界区见容易触发错误的判定
- 滑动窗口
- 漏桶
  - 在一定时间内，允许通过恒定数量的请求，常用于请求第三方
- 令牌
  - 漏桶的优化版，允许一定时间内的突发流量，适合微服务

## 单机限流与全局限流

- 全局限流
  - 一组服务，通过外部存储对集群整体流量做限流，适合有 DB 有吞吐量限制的场景，扩容不方便
- 单机限流
  - 通过对单机的限流，达到对集群的限流

## 熔断

- 状态
  - close：初始为关闭状态
  - open：熔断中
  - halfopen：经过一段可配时间，从 open 变为 halfopen，按照线性方式放行流量，
- 主要配置
  - 滑动窗口时间：如 10s
  - 触发条件：如错误码
