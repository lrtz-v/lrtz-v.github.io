---
template: main.html
tags:
  - Service Mesh
  - 注册中心
  - 负载均衡
  - 服务治理
  - 网关
  - 配置中心
  - Trace
  - Envoy
---

# 服务注册中心

## 健康检查设计

- 服务主动探活
  - 服务定时发送租约信息给注册中心
  - 写操作压力大、主动租约不能保证服务健康
- 注册中心发起健康检查
  - 可能会发生旧服务重新激活的问题
  - 可用 envory 做服务名检查
- 调用房的负载均衡器进行健康检查
  - 如 grpc 的自动摘除节点
  - IP 重用产生脏数据

## 注册中心的选型

| 特征         | Nacos | Eureka | Zookeeper  | Consul | Etcd   |
| ------------ | ----- | ------ | ---------- | ------ | ------ |
| 一致性协议   | AP    | AP     | CP         | CP     | CP     |
| 健康检查     | 多种  | TTL    | Keep Alive | 多种   | TTL    |
| 网络异常保护 | 支持  | 支持   | 不支持     | 支持   | 不支持 |
| 实现语言     | Java  | Java   | Java       | Go     | Go     |

## 搭建高可用的注册中心

- Q1：注册中心故障了，服务是否还能正常访问？
  - 只要服务缓存了服务数据，影响面可控
  - 服务无法扩容
- Q2：注册中心因为高负载，推送了异常的数据，服务是否还能正常访问？
  - 在客户端的服务发现 SDk 中加入自我保护机制，当服务的节点数量下降超过一定阈值，就进入保护机制，放弃使用新推送过来的服务注册信息
- Q3：新加入的机器，出现了网络联通性问题（注册中心与服务网络正常，服务之间网络异常），应该怎样应对？
  - 在负载均衡中加入被动健康检查（节点熔断）和主动健康检查，主动剔除失效的节点
- Q4：服务是否应该完全信任注册中心推送的数据？

  - 相比注册中心数据，更信任本地数据
  - 使用 Envoy 2X2 矩阵来决定节点是否应该路由

    | 发现状态 | 健康检查成功 | 健康检查失败 |
    | -------- | ------------ | ------------ |
    | 发现     | 路由         | 不路由       |
    | 未发现   | 路由         | 不路由，删除 |

- Q5：服务发布后，造成 N\*M 次事件通知，形成广播风暴，该如何解决？
  - 将消息推送合并

## Service Mesh 中的注册中心

- 涉及跨集群访问
  - EDS 服务发现规范
- 与 K8S 集群结合
  - 需要注意节点 IP 变化的问题

## Service Mesh 中的注册中心的优势

- 由 sidercar 代理注册，减少服务开发量，可以控制注册 meta 信息一致化
- 通过控制面聚合多种、多个注册中心数据，降低单一注册中心的读写压力，更容易水平扩展
- 通过 sidercar 提供服务正确性检查功能，通过 header 中增加服务名与本地服务名称做校验

# 负载均衡

## 应该选择什么样的负载均衡器

- 硬件负载均衡器

  - F5
  - Citrix NetScaler
  - radware
  - Array

- DNS 负载均衡
- 软件负载均衡

  - Nginx（七层）
  - HAProxy（七层）
  - LVS（四层）

- 程序内负载均衡：讲负载均衡放在服务程序内部
- Service Mesh 负载均衡：利用 sidercar 做负载均衡，属于软件负载均衡的一种

## 负载算法

- Round Robin：轮训算法，适合节点权重一致的场景
- Weighted Round Robin：将权重大的节点分散开，取最大公约数，做简单轮训；在服务重启时会出现请求集中的现象
- Weighted Random：通过随机的方式进行负载，配合二分查找，将时间复杂度降低到 logn，对后端节点非常均衡
- Two Random Choices：通过两次随机算法，选取两个节点，再对比两个节点的负载、延时等信息，选取最优的一个
- Stick Session：同一个客户端，负载到的节点相同，负载不均衡，慎用

## 服务发现后如何实现节点保护

- 主动健康检查
  - 在负载均衡中主动健康检查。
  - 会产生大量无用 ping；建议选择获取过少节点时才触发，如当前节点数比十五分钟前的少 20%时触发
- 恐慌阈值
  - 当健康检查后，可用的节点依然小于阈值（10%），则忽略健康检查的结果，把流量负载到所有节点，包括异常的节点，保证理由的均衡，健康的节点也不会因为激增的流量而雪崩
- 被动健康检查
  - 通过正常的流量判断节点是否正常，也就是利用节点熔断器
  - 将 499 及以上的错误码认为是后端服务异常，统计 10 秒滑动窗口内异常请求占比

## 节点染色

- 利用节点上的标签进行流量路由；把某一类节点打上相同标签，负载均衡器根据相同标签分发流量
  - 金丝雀发布、A/B 测试、故障演练、流量分区等
- 如何操作
  - 在网关层，根据 header 信息或者权重对流量进行染色
  - 在注册中心中也要写入对应的 metaData 信息，用户在负载均衡层进行流量过滤

## 地域优先访问

- 在 Envoy 中被称为 zone 感知路由
- 名词解释
  - 始发集群：调用方，Client 的服务节点集群
  - 上游集群：被调用方，Server 服务节点集群
  - zone：区域（Region）和可用区（Availability Zone）
- zone 感知路由，会根据始发集群的所在区节点数量和目标集群的所在区节点数量，动态计算一个相对的比例；通过动态计算上下游的节点数，将流量正确的路由到各个分区，避免上游集群承受过大的访问量而崩溃
  - 始发集群的本地 zone 节点数量小于或等于上游服务节点数量，直接全量负载到上游集群 zone，并计算剩余流量比例用于服务其它 zone
  - 始发集群的本地 zone 节点数量大于上游服务节点数量，需要将剩余流量路由到其它上游服务 zone
- 节点缩量变化，可能会导致瞬间的流量不均衡
  - P2C（pick off to chioce）

## 延时、负载加权（EWMA）

- 基于 P2C 算法的负载均衡，如 Envoy、Nginx、Linkerd
- 负载率: client_success/server_cpu\*math.Sqrt(latency+1)\*(inflight+1)
  - client_success：客户端请求成功率
  - server_cpu：每次请求 response 的 header 携带的瞬时 cpu
  - latency：客户端计算的延时
  - inflight：正在发送中的请求数量
- EWMA 计算延时和客户端成功率

## 负载均衡中的常见问题

- Q1：为什么四层负载均衡流量不均匀？
  - 四层负载均衡是基于连接做负载均衡，由于连接保持，流量会始终打到固定节点；新加入的节点，往往需要较长时间才有流量
- Q2：负载流量一致，后端服务的负载就一致吗？
  - 因为服务器硬件差异，后端服务的负载很难一致
- Q3：节点下线后，如何及时摘除节点？
  - 注册中心异步推送存在延迟，可通过 upsteam 节点，返回健康检查信息检查失败的头信息，快速摘掉节点

# 使用路由器模块（软路由）针对不同流量实现不同的路由策略

## 名词解释

- Name：主要对应 Listener 的名称，用端口座位名字，可以保证唯一性
- Domains：在 Envoy 中会先做一次初步的过滤，这层过滤就是服务域名。主要通过字符串匹配
- routes：服务路由配置，针对要访问的服务设置多条路由配置
  - match：路由匹配，匹配此字段到对应的路由设置，可以设置如 path、pathPrefix、header 等
  - route：基本规则设置，如 Cluster 字段代表路由对应的服务名
  - per_filter_config：路由对应的中间件配置，用于服务治理、限流、熔断等

## 路由中间件

- 把中间件放在路由层

## 服务重写

- sidercar 的设计模式，保证基础设施和业务独立演化

## RDS 服务发现服务

- 动态发现路由变化，及时更新路由策略

# 服务治理

## 限流算法

- 计数器：记录一定时间内的请求量
  - 在临界区见容易触发错误的判定
- 滑动窗口
- 漏桶
  - 在一定时间内，允许通过恒定数量的请求，常用于请求第三方
- 令牌
  - 漏桶的优化版，允许一定时间内的突发流量，适合微服务

## 单机限流与全局限流

- 全局限流
  - 一组服务，通过外部存储对集群整体流量做限流，适合有 DB 有吞吐量限制的场景，扩容不方便
- 单机限流
  - 通过对单机的限流，达到对集群的限流

## 熔断

- 状态
  - close：初始为关闭状态
  - open：熔断中
  - halfopen：经过一段可配时间，从 open 变为 halfopen，按照线性方式放行流量，
- 主要配置
  - 滑动窗口时间：如 10s
  - 触发条件：如错误码

# 连接池

- HTTP 连接池
- HTTP/2 连接池

# 网关

## API-Gateway

- 提供统一的流量入口
- 业务聚合
- 协议转换，http -> grpc/thrift
- 中间件策略，限流熔断等
- 安全认证
- 证书管理，如 https 证书拆卸、管理等

## 常用网关

- Kong（lua + OpenResty）
  - Service：后端服务
  - router：路由
  - Admin Api：内部管理接口
  - Plugins：插件
  - Load Balancing：负载，DNS 和内置负载均衡器
  -
- Zuul（Java）
- Traefik（Go）

## 双重网关（系统网关和业务网关）

- 分别面向运维和开发
- 业务网关还可以根据业务维度拆分，防止单一网关故障带来全站不可用

# 配置中心

## 配置中心的优势

- 减少发版次数
- 提升安全性

## 配置中心的特性

- 实时感知配置变更
  - 实时性要求不高，要求最终一致
- 变更频率低
- 安全性要求高，如证书等
- 变更审计，可追踪
- 灰度发布
- 变更回滚
- 弱依赖
  - 通过 SDK 增加配置缓存，即便配置中心服务异常，也不影响当前服务使用
- 图形操作界面

## 配置中心选型

- Etcd
- Apollo
- Confd

## 配置中心的实时变更

- 长连接 watch，TPC/gRPC
- HTTP 长轮询，HTTP KeepAlive
- 定时同步
- 长连接 watch + 定时轮训

## Service Mesh 的配置中心

- 服务治理及中配置
  - 系同层面的配置需要抽象出来，形成同意的数据结构供控制面使用
- 平台化
  - 提供 SRE 操作平台

# Trace

## 可观测组件

- 链路追踪
  - 记录调用链路信息
- Metrics 监控指标
  - 记录服务状态
- 日志分析
  - 用于排查问题

## Trace 链路追踪原理

- TraceId：全局唯一的调用标识
- span、parentSpan：表示调用次数

## 常见链路追踪系统

- Zipkin，由 Twitter 开源
- Jaeger，由 Uber 开源

## Trace 日志落盘

- SDK
- 采样率

# Metrics

## 常用的 Metrics 系统

- StatsD + Graphite
- influDB + Telegraf
- Prometheus

## Prometheus 的 Metrics 类型

- Counter
  - 累加值，适合统计 QPS 等
- Gauge
  - 适合记录瞬时值，如统计熔断、限流事件
- Histogram
  - 适合统计 99 延时等信息，适合高性能的场景使用
- Summary
  - 类似于 Histogram，性能比 Histogram 弱，但更精准

# Service Mesh 选型

- https://servicemesh.es

| 解决方案 | Istio      | Linkerd2       | SOFAMaesh                      | Kuma           | Consul          | Traefik              |
| -------- | ---------- | -------------- | ------------------------------ | -------------- | ----------------------- | -------------------- |
| 服务代理 | Envoy      | linkerd-proxy  | Traefik                        | Envoy          | Envoy                   | Envoy                |
| 开发语言 | GO、C++    | Go、Rust       | Go                             | Go、C++        | Go、C++                 | C++                  |
| 平台     | K8S        | K8S            | K8S、VM                        | K8S、VM        | K8S                     | K8S、VM              |
| 协议支持 | 多种 RPC   | 多种 RPC       | Http/gRPC                      | 多种 RPC       | 多种 RPC                | 多种 RPC             |
| 负载均衡 | 多种       | P2C            | WRR                            | 多种           | 多种                    | 多种                 |
| 优点     | 知名、成熟 |                |                                | 多平台支持較好 | 适合在 consul 上扩展    | 云厂商保证服务稳定性 |
| 缺点     | 版本变化大 | 服务治理不完善 | 节点部署方式，不适合大规模应用 | 冷门           | 不支持限流、绑定 consul | 绑定云服务商         |

# 数据面-Envoy

- 专为大型现代 SOA 架构设计的 L7 代理和通信总线，可以作为数据面、入口网关使用，通过 xDS API 控制 Envoy 的监听、路由、负载均衡等行为

## 核心组件

- Iptable：通过 Iptable 劫持，将入口和出口的流量全转发到 Envoy 上
- Listener：通过建立多个监听器提供不同的服务，例如，监听两个的端口分别负责 sidercar 模式的出流量和入流量。如过提供不同的协议，Envoy 也会建立不同的端口提供服务
- Worker：每个 Listener 维护一个对应的 worker pool，Envoy 为每个逻辑处理器创建一个 worker 线程，当我们在一个新的端口启动一个新的 server 时，Envoy 也会创建对应的 worker 线程。太多的 worker 线程不一定是好事，特别的在 sidercar 模式
- Filters：提供四层、七层的流量过滤，支持服务治理
- Cluster Manager：流量经过 Router 识别出需要转发的 Cluster，通过 Cluster Manager 进行服务发现和负载均衡
- Upstream：维护 Endpoint 的连接池，通过负载均衡器将流量转发到合适的 Endpoint 上

## Envoy 作为 sidercar 的使用

- Envoy 作为 Sidercar 使用时，需要和服务部署痛一台机器或者同一个 pod 中，当用户访问其他服务时，流量会自动被劫持到 Envoy 中
- 流程
  - 通过 Iptable 对流量进行劫持，将流量转发到出流量端口
  - Envoy 先根据 virtual hosts 匹配，再通过路由匹配，发现路由对应的 Cluster，通过服务发现找到 Cluster 对应的 Endpoint，将流量转发到目标 pod
  - Pod 内，通过 Iptable 对入流量劫持，将流量劫持到 Envoy 的入端口
  - Envoy 将本地流量转发到对应的 pod 内本地地址

## Envoy 配置

- 静态配置
  - 手动填写
- 动态配置
  - 通过 xDS API 获取配置

## Envoy 边缘代理网关

- 边缘代理网关：负责网格出口与入口流量负载均衡的特殊数据面，它不以 Sidecar 的形式，而是以独立 Pod 的形式部署在您的集群内

- 特殊设置
  - HTTP 头清理
    - 如 x-forward-for
  - 超时控制
    - 连接超时：Envoy 为 HTTP 服务提供空闲连接超时时间的设置，默认一小时
    - 流超时：流时 HTTP/2 中的概念，Envoy 通过将 HTTP 连接对应到流模式，统一进行处理
    - 路由超时：为某些请求设置特殊的配置
  - 连接限制
    - Envoy 可以针对全局或者监听器设置连接限制，可以根据服务峰值设置合理的连接限制

# xDS-控制面与数据面的通信

- xDS API：一套可扩展的通用微服务控制 API，这些 API 可以做到服务发现、路由、集群发现等功能。xDS 中每种类型对应一个发现的资源，这些类型数据存储在 xDS 协议的 Discovery Request 和 Discovery Response 的 TypeUrl 字段中，格式为：type.googleapis.com/**\<resource type>**
- 资源类型
  - LDS：监听器发现服务，对应 Listener 数据类型
  - CDS：集群发现服务，对应 Cluster 数据类型
  - RDS：路由发现服务，对应 Route 数据类型
  - EDS：节点发现服务，包含服务名、节点信息、LB 策略等
  - SDS：密钥发现服务，用于证书发现

## todo：gRPC 流式订阅

# Engress & Ingress

## Ingress

- K8S 集群外访问 pod，需要通过 NodePort 和 Ingress
  - NodePort：通过暴露 node 端口，提供访问 K8S Service 的入口
  - Ingress：通过 ingress 路由，将请求转发到 K8S Service 上，内部依然是使用的 ClusterIP，需要通过 IPVS 四层转发

## Istio Gateway

- 允许外部流量访问内部服务，代替 Ingress；Gateway 只需要配置流量转发即可

## Egress 出口网关

### K8S Engress

- 通过 IP 地址或端口层面（OSI 第 3 层或第 4 层）控制网络流量

### Istio Egress

- Istio Egress 本质上是一个 Envoy Proxy，通过 Envoy 的七层负载，提供路由策略
